{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of each action: 4\n",
      "episode done  [False]\n",
      "odict_keys(['normalizer.weight', 'normalizer.bias', 'normalizer.running_mean', 'normalizer.running_var', 'normalizer.num_batches_tracked', 'layers.0.weight', 'layers.0.bias', 'layers.1.weight', 'layers.1.bias', 'output.weight', 'output.bias'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home_01/f20150198/miniconda2/envs/python3/lib/python3.5/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 30.58999931626022 [30.58999932]\n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch\n",
    "env = UnityEnvironment(file_name='Reacher_Linux_NoVis/Reacher.x86_64')\n",
    "from ddpg_agent import Agent\n",
    "brain_name = env.brain_names[0]\n",
    "num_agents =1\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "\n",
    "brain = env.brains[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "print(\"episode done \",env_info.local_done)\n",
    "# examine the state space \n",
    "state = env_info.vector_observations\n",
    "state_size = state.shape[1]\n",
    "agent = Agent(state_size=state_size, action_size=action_size, random_seed=3)\n",
    "#agent.actor_local\n",
    "agent.reset()\n",
    "agent.critic_local.eval()\n",
    "agent.actor_local.eval()\n",
    "print(torch.load(\"checkpoint_critic_temp.pth\", map_location=lambda storage, loc: storage).keys())\n",
    "agent.critic_local.load_state_dict(torch.load(\"checkpoint_critic.pth\"))\n",
    "agent.actor_local.load_state_dict(torch.load(\"checkpoint_actor.pth\"))\n",
    "while True:\n",
    "    action = agent.act(state)\n",
    "    env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    #print(rewards)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    state = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)),scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of each action: 4\n",
      "episode done  [False]\n",
      "hello\n",
      "avg score 38.629999136552215\n",
      "Episode 1\tAverage Score: 38.62999914[38.629999136552215]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import deque\n",
    "from ddpg_agent import Agent\n",
    "from unityagents import UnityEnvironment\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "env = UnityEnvironment(file_name='Reacher_Linux_NoVis/Reacher.x86_64')\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "print(\"episode done \",env_info.local_done)\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "agent = Agent(state_size=state_size, action_size=action_size, random_seed=3)\n",
    "agent.critic_local.load_state_dict(torch.load(\"checkpoint_critic.pth\"))\n",
    "agent.actor_local.load_state_dict(torch.load(\"checkpoint_actor.pth\"))\n",
    "#scores = np.zeros(1)                          # initialize the score (for each agent)\n",
    "print(\"hello\")\n",
    "def ddpg(n_episodes=5000, max_t=2000, print_every=100):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        #/print(\"mello\")\n",
    "        agent.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment\n",
    "        #print(\"kello\")\n",
    "        state = env_info.vector_observations                 # get the current state (for each agent)\n",
    "        score = np.zeros(1)\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state)\n",
    "            env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "            next_state = env_info.vector_observations         # get next state (for each agent)\n",
    "            reward = env_info.rewards                        # get reward (for each agent)\n",
    "            done = env_info.local_done                        # see if episode finished\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done[0]:\n",
    "                break\n",
    "        avg_score = np.mean(score)\n",
    "        print(\"avg score\",avg_score)\n",
    "        scores_deque.append(avg_score)\n",
    "        scores.append(avg_score)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.8f}'.format(i_episode, np.mean(scores_deque)), end=\"\")\n",
    "        if np.mean(scores_deque) >= 30:\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor_temp.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic_temp.pth')\n",
    "            break\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            `\n",
    "    return scores\n",
    "\n",
    "scores = ddpg()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "print(scores)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "plt.savefig(\"graph.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b7\u001b[?47h\u001b[?1h\u001b=\r",
      "Found path: /home_01/f20150198/projects/deep-reinforcement-learning/p2_continuou \bs-control/Reacher_Linux_NoVis/Reacher.x86_64\r\n",
      "Mono path[0] = '/home_01/f20150198/projects/deep-reinforcement-learning/p2_conti \bnuous-control/Reacher_Linux_NoVis/Reacher_Data/Managed'\r\n",
      "Mono config path = '/home_01/f20150198/projects/deep-reinforcement-learning/p2_c \bontinuous-control/Reacher_Linux_NoVis/Reacher_Data/MonoBleedingEdge/etc'\r\n",
      "Preloaded 'libgrpc_csharp_ext.x64.so'\r\n",
      "Unable to preload the following plugins:\r\n",
      "        libgrpc_csharp_ext.x86.so\r\n",
      "Logging to /home_01/f20150198/.config/unity3d/Unity Technologies/Unity Environme \bnt/Player.log\r\n",
      "Size of each action: 4\r\n",
      "episode done  [False]\r\n",
      "hello\r\n",
      "\u001b[7m^M\u001b[mEpisode 1     Average Score: 0.00000000\u001b[7m^M\u001b[mEpisode 2    Average Score: 0.0000000 \b0\u001b[7m^M\u001b[mEpisode 3    Average Score: 0.41666666\u001b[7m^M\u001b[mEpisode 4    Average Score: 0.4374999 \b9\u001b[7m^M\u001b[mEpisode 5    Average Score: 0.34999999\u001b[7m^M\u001b[mEpisode 6    Average Score: 0.3816666 \b6\u001b[7m^M\u001b[mEpisode 7    Average Score: 0.43999999\u001b[7m^M\u001b[mEpisode 8    Average Score: 0.4837499 \b9\u001b[7m^M\u001b[mEpisode 9    Average Score: 0.48444443\u001b[7m^M\u001b[mEpisode 10   Average Score: 0.4689999 \b9\u001b[7m^M\u001b[mEpisode 11   Average Score: 0.50999999\u001b[7m^M\u001b[mEpisode 12   Average Score: 0.5283333 \b2\u001b[7m^M\u001b[mEpisode 13   Average Score: 0.50769230\u001b[7m^M\u001b[mEpisode 14   Average Score: 0.5199999 \b9\u001b[7m^M\u001b[mEpisode 15   Average Score: 0.49666666\u001b[7m^M\u001b[mEpisode 16   Average Score: 0.5249999 \b9\u001b[7m^M\u001b[mEpisode 17   Average Score: 0.58058822\u001b[7m^M\u001b[mEpisode 18   Average Score: 0.5683333 \b\u001b[7mlog.out\u001b[m\u001b[K"
     ]
    }
   ],
   "source": [
    "!less log.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
