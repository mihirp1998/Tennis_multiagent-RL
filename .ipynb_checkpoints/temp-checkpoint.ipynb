{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from unityagents import UnityEnvironment\n",
    "# from config import Config\n",
    "# from network import Actor, Critic\n",
    "# from memory import ReplayBuffer\n",
    "# from noise import OUNoise\n",
    "# from agent import DDPGAgent\n",
    "from mulagent import MultiAgent\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "env = UnityEnvironment(file_name='Tennis_Linux_NoVis/Tennis.x86_64')\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "a = open(\"file.txt\",'w+')\n",
    "\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "print(\"episode done \",env_info.local_done)\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "agent = MultiAgent(state_size=state_size, action_size=action_size, num_agents=len(env_info.agents))\n",
    "#scores = np.zeros(1)                          # initialize the score (for each agent)\n",
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=5000, max_t=2000, print_every=100):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        #/print(\"mello\")\n",
    "        agent.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment\n",
    "        #print(\"kello\")\n",
    "        state = env_info.vector_observations                 # get the current state (for each agent)\n",
    "        score = np.zeros(len(env_info.agents))\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state)\n",
    "            env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "            next_state = env_info.vector_observations         # get next state (for each agent)\n",
    "            reward = env_info.rewards                        # get reward (for each agent)\n",
    "            done = env_info.local_done                        # see if episode finished\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "        avg_score = np.mean(score)\n",
    "        scores_deque.append(avg_score)\n",
    "        scores.append(avg_score)\n",
    "        a.write('\\rEpisode {}\\tAverage Score: {:.8f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        a.flush()\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.8f}'.format(i_episode, np.mean(scores_deque)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            torch.save(agent.ddpg_agents[0].actor_local.state_dict(), 'checkpoint_inter_actor.pth')\n",
    "            torch.save(agent.ddpg_agents[0].critic_local.state_dict(), 'checkpoint_inter_critic.pth') \n",
    "        if np.mean(scores_deque) >= 0.5:\n",
    "            torch.save(agent.ddpg_agents[0].actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.ddpg_agents[0].critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            break\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b7\u001b[?47h\u001b[?1h\u001b=\r",
      "\r",
      "\u001b[K1, -0.6949999844655395, -0.6949999844655395, -0.6949999844655395, -0.69499998446 \b55395, -0.6899999845772982, -0.6949999844655395, -0.6399999838322401, -0.6949999 \b844655395, -0.6949999844655395, -0.6349999839439988, -0.6299999840557575, 0.0050 \b00022239983082, -0.25499997939914465, -0.3499999810010195, -0.23999997973442078, \b 0.04000002332031727, -0.6949999844655395, -0.6949999844655395, -0.6949999844655 \b395, -0.6899999845772982, -0.6899999845772982, -0.6399999838322401, -0.694999984 \b4655395, -0.6949999844655395, -0.6949999844655395, -0.6949999844655395, -0.69499 \b99844655395, -0.6899999845772982, -0.6949999844655395, -0.6949999844655395, -0.6 \b949999844655395, -0.6949999844655395, -0.6949999844655395, -0.6949999844655395,  \b-0.6949999844655395, -0.6949999844655395, -0.6949999844655395, -0.68999998457729 \b82, -0.6899999845772982, -0.6449999837204814, -0.6949999844655395, -0.6249999841 \b675162, -0.6949999844655395, -0.6949999844655395, -0.6949999844655395, -0.694999 \b9844655395, 0.7700000274926424, 0.6600000262260437, 0.6800000257790089, 1.095000 \b029541552, 1.0800000298768282, 1.095000029541552, 1.2150000305846334, 0.74500002 \b61887908, 1.4000000320374966, 1.1400000303983688, 1.1050000293180346, 1.35000003 \b40864062, 1.4600000381469727, 1.3600000366568565, 1.3050000360235572, 1.01000003 \b14414501, 1.4150000372901559, 1.4050000375136733, 1.6750000407919288, 1.40500003 \b75136733, 1.4200000371783972, 1.2600000351667404, 1.9650000417605042, 2.26000004 \b63426113, 2.5950000481680036, 2.935000052675605, 3.0950000546872616, 3.030000051 \b483512, 3.3750000577419996, 3.055000053718686, 2.995000052265823, 3.120000054128 \b468, 3.060000052675605, 2.945000051520765, 2.715000049211085, 2.535000049509108, \b 2.605000050738454, 2.7050000494346023, 2.210000043734908, 2.9750000527128577, 2 \b.5450000474229455, 2.665000048466027, 2.7550000501796603, 2.6300000501796603]\r\n",
      "\u001b[7mnohup.out (END)\u001b[m\u001b[K"
     ]
    }
   ],
   "source": [
    "!less +G nohup.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = ddpg()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "print(scores)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.savefig(\"graph.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
